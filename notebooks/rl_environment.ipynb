{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fc1c8dc",
   "metadata": {},
   "source": [
    "## Custom Gymnasium Enviroment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7df1201",
   "metadata": {},
   "source": [
    "Import whats needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecca06ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f31a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we convert the jupiter file to a python script since it is easier to handle \n",
    "!jupyter nbconvert --to script --output rl_environment rl_environment.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30f4f7d",
   "metadata": {},
   "source": [
    "Trading Enviroment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e3fe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym         # Gymnasium is a library for building RL environments\n",
    "                                # It provides a standard interface so RL algorithms (like PPO from Stable Baselines3) can interact with your environment.\n",
    "from gymnasium import spaces    # spaces defines the action space and observation space for your environment\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data: pd.DataFrame, window_size=50):\n",
    "        super().__init__()                              # initialize the base class\n",
    "        self.data = data.reset_index(drop=True)         # reset index for easier slicing and store it in self.data\n",
    "        self.window_size = window_size                  # number of previous candles to include in the observation\n",
    "        self.current_step = window_size                 # start after the initial window\n",
    "\n",
    "        # Observation: last N candles (OHLC + Volume)\n",
    "        self.observation_space = spaces.Box(            # description of the observation space is given\n",
    "            low=-np.inf, high=np.inf,                   # low and high values for each element in the observation are unbounded / infinite\n",
    "            shape=(window_size, self.data.shape[1]),    # shape of the observation (window_size rows, number of features columns)\n",
    "            dtype=np.float32                            # data type of the observation elements\n",
    "        )\n",
    "        # Actions: 0 = Hold, 1 = Buy, 2 = Sell\n",
    "        self.action_space = spaces.Discrete(3)          # actually, there is not much we can do\n",
    "\n",
    "    def reset(self, seed=None, options=None):           # reset the environment to initial state\n",
    "        super().reset(seed=seed)                        # call the base class reset method\n",
    "        self.current_step = self.window_size            # bring current step back to initial position\n",
    "        return self._get_observation(), {}              # return the last known observation and an empty info dict\n",
    "                                                        #   -> dict can be used to pass additional information like debugging info                                               \n",
    "\n",
    "    def step(self, action):                             # do one step of the training\n",
    "        reward = self._calculate_reward(action)         # calculate reward based on action taken - in the end the |profit/loss|\n",
    "        self.current_step += 1                          # move to the next time step\n",
    "        done = self.current_step >= len(self.data) - 1  # episode is done if we reach the end of the data\n",
    "        return self._get_observation(), reward, done, False, {} \n",
    "\n",
    "    def _get_observation(self):\n",
    "        obs = self.data.iloc[self.current_step - self.window_size:self.current_step].values # Return last N rows as observation \":\" is NOT a division\n",
    "        return obs.astype(np.float32)                   # return the last <windwow_size> observations as float32 numpy array\n",
    "\n",
    "    def _calculate_reward(self, action):                # the REWARD is calculated based on the difference in closing prices\n",
    "        if self.current_step + 1 >= len(self.data):     # check if we are at the end of the data\n",
    "            print(\"End of data reached - we should never get here\")\n",
    "            return 0.0\n",
    "        price_diff = self.data['Close'].iloc[self.current_step + 1] - self.data['Close'].iloc[self.current_step]\n",
    "        if action == 1:   # BUY <-----------------------\n",
    "            return price_diff\n",
    "        elif action == 2: # SELL <----------------------\n",
    "            return -price_diff\n",
    "        else:             # HOLD <----------------------\n",
    "            return 0.0\n",
    "\n",
    "    \n",
    "    def render(self, action=None, reward=None):  # render the current state of the environment\n",
    "        msg = f\"Step: {self.current_step}, Close Price: {self.data['Close'].iloc[self.current_step]}\"\n",
    "        if action is not None:  # if an action was taken, include it in the message\n",
    "            msg += f\", Action: {action}\"\n",
    "        if reward is not None:  # if a reward was given, include it in the message\n",
    "            msg += f\", Reward: {reward:.4f}\"\n",
    "        print(msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a17e318",
   "metadata": {},
   "source": [
    "Get the data and normalize it with Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7a0ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Load raw data\n",
    "data = pd.read_csv('../data/Candlestick_01jan2000_31dec2024.csv', parse_dates=['date'], index_col='date')\n",
    "\n",
    "# Split into train/test\n",
    "split_idx  = int(len(data) * 0.8)\n",
    "train_data = data.iloc[:split_idx]\n",
    "test_data  = data.iloc[split_idx:]\n",
    "\n",
    "# Fit scaler on training data only\n",
    "scaler       = StandardScaler()\n",
    "scaled_train = scaler.fit_transform(train_data) # train the scaler on train data\n",
    "scaled_test  = scaler.transform(test_data)      # run the scaler on the rest\n",
    "\n",
    "joblib.dump(scaler, '../models/scaler.pkl')     # Save scaler for future use\n",
    "\n",
    "# Convert back to DataFrame\n",
    "scaled_train_df = pd.DataFrame(scaled_train, columns=data.columns, index=train_data.index)\n",
    "scaled_test_df  = pd.DataFrame(scaled_test,  columns=data.columns, index=test_data.index )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4327f34b",
   "metadata": {},
   "source": [
    "PPO training and evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115f0dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Create environment\n",
    "env = TradingEnv(scaled_train_df, window_size=50)\n",
    "\n",
    "# Train PPO agent\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)    # create PPO (Proximal Policy Optimization) model with MLP policy\n",
    "model.learn(total_timesteps=100_000)        # train the model for 100,000 timesteps\n",
    "\n",
    "model.save(\"../models/ppo_trading\")         # save the trained model (the model is now trained until the NOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0a9d92",
   "metadata": {},
   "source": [
    "Now let's test the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2fa4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test-Umgebung erstellen\n",
    "test_env = TradingEnv(scaled_test_df, window_size=50)\n",
    "\n",
    "# Evaluation\n",
    "obs, info = test_env.reset()\n",
    "total_reward = 0\n",
    "steps = 0\n",
    "\n",
    "while True:\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, done, _, _ = test_env.step(action)\n",
    "    total_reward += reward\n",
    "    steps += 1\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(f\"Total Reward on Test Data: {total_reward:.4f}\")\n",
    "print(f\"Steps: {steps}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
